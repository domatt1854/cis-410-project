{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/bhalla/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_csv('raw_data/computerscience_hot_posts.csv')\n",
    "df_train = pd.read_csv('raw_data/Reddit_Data.csv')\n",
    "### There are empty rows in 'raw_data/Reddit_Data.csv'\n",
    "df_train = df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "positive_words = [word for word, tag in filter(\n",
    "                  skip_unwanted,\n",
    "                  nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    "                 )]\n",
    "negative_words = [word for word, tag in filter(\n",
    "                  skip_unwanted,\n",
    "                  nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    "                  )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def get_features(dataset, count):\n",
    "    def intersection(list1,list2):\n",
    "        x = set(list1)\n",
    "        y = set(list2)\n",
    "        z = x.intersection(y)\n",
    "        return len(z)\n",
    "    \n",
    "    def get_simple_features(text):\n",
    "#         print(text)\n",
    "        features = {}\n",
    "        \n",
    "        # Feature #1 - verbosity\n",
    "        features['verbosity'] = len(text)\n",
    "        \n",
    "#         # Feature #2 - lexical word choice\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        features['vader(pos)'] = scores['pos']\n",
    "        features['vader(neg)'] = scores['neg']\n",
    "        features['vader(neu)'] = scores['neu']\n",
    "        features['vader(compound)'] = scores['compound']\n",
    "        \n",
    "#         # Feature #3 - Positive and Negative Words Frequency\n",
    "        words = word_tokenize(text)\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        pos = intersection(words,positive_words)\n",
    "        neg = intersection(words,negative_words)\n",
    "        features['num_pos'] = pos\n",
    "        features['num_neg'] = neg\n",
    "        try:\n",
    "            features['tone'] = (pos-neg) / (pos+neg)\n",
    "        except:\n",
    "            features['tone'] = 0\n",
    "        return features \n",
    "    \n",
    "    \n",
    "    features = [(get_simple_features(row['clean_comment']), row['category']) \n",
    "                    for index, row in dataset.iterrows()]\n",
    "    \n",
    "    # Feature #4 - TFIDF\n",
    "    train_data, test_data = dataset[:count], dataset[count:]\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features = 10,\n",
    "                                       stop_words='english',\n",
    "                                       use_idf=True, \n",
    "                                       norm='l2', \n",
    "                                       smooth_idf=True) \n",
    "    tfidf_train_vectors = tfidf_vectorizer.fit_transform(train_data['clean_comment']).toarray()\n",
    "    tfidf_test_vectors = tfidf_vectorizer.transform(test_data['clean_comment']).toarray()\n",
    "    tfidf_vectors = np.vstack([tfidf_train_vectors,tfidf_test_vectors])\n",
    "    \n",
    "    for idx in range(len(features)):\n",
    "        row = features[idx]\n",
    "        for column in range(tfidf_vectors.shape[1]):\n",
    "            feature_name = 'tfidf_' + str(column)\n",
    "            row[0][feature_name] = tfidf_vectors[idx][column]\n",
    "        features[idx] = row    \n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_train_test_split(dataset, split_percentage=0.9):\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    count = int(len(dataset) * split_percentage)\n",
    "    \n",
    "    feature_set = get_features(dataset, count)\n",
    "    \n",
    "    train_set, test_set = feature_set[:count], feature_set[count:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, model):\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(dataset)\n",
    "    \n",
    "    return nltk_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataset(dataset, model):\n",
    "    ## 0 Indicating it is a Neutral Tweet/Comment\n",
    "    ## 1 Indicating a Postive Sentiment\n",
    "    ## -1 Indicating a Negative Tweet/Comment\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        try:\n",
    "            res.append(model.classify(get_features(row['text'])))\n",
    "        except:\n",
    "            res.append(None)\n",
    "            \n",
    "    result_df = dataset\n",
    "    result_df['sentiment'] = res\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = data_train_test_split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors model Accuracy: 0.6314939434724092\n",
      "Decision Tree model Accuracy: 0.6742934051144011\n",
      "Random Forest model Accuracy: 0.6877523553162853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhalla/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model Accuracy: 0.6648721399730821\n",
      "SGD Classifier model Accuracy: 0.642799461641992\n"
     ]
    }
   ],
   "source": [
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier',\n",
    "         #'Support Vector Classifier'\n",
    "        ]\n",
    "\n",
    "random_state = 1234\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(weights='distance', n_neighbors=60, p=1),\n",
    "    DecisionTreeClassifier(min_samples_split=100, min_samples_leaf=35, max_depth=8),\n",
    "    RandomForestClassifier(min_samples_split=100, min_samples_leaf=50, n_estimators=100, max_depth=20, random_state=random_state),\n",
    "    LogisticRegression(),\n",
    "    CalibratedClassifierCV(SGDClassifier(max_iter=100)),\n",
    "    # SVC(kernel='linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "df_result = {}\n",
    "\n",
    "for name, model in models:\n",
    "    classfier = train_model(train_set, model)\n",
    "    accuracy = nltk.classify.accuracy(classfier, test_set)\n",
    "    result = label_dataset(df_predict, classfier)\n",
    "    df_result[name] = result\n",
    "    print(\"{} model Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble: Voting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhalla/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier (hard) model Accuracy: 0.6767160161507403\n"
     ]
    }
   ],
   "source": [
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble_hard = SklearnClassifier(VotingClassifier(estimators=models, voting='hard', n_jobs=-1))\n",
    "nltk_ensemble_hard.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble_hard, test_set)\n",
    "print(\"Voting Classifier (hard) model Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = label_dataset(df_predict, nltk_ensemble_hard)\n",
    "df_result['Voting Hard'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhalla/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier (soft) model Accuracy: 0.677792732166891\n"
     ]
    }
   ],
   "source": [
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble_soft = SklearnClassifier(VotingClassifier(estimators=models, voting='soft', n_jobs=-1))\n",
    "nltk_ensemble_soft.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble_soft, test_set)\n",
    "print(\"Voting Classifier (soft) model Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = label_dataset(df_predict, nltk_ensemble_soft)\n",
    "df_result['Voting Soft'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
